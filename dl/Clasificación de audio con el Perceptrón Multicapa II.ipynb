{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de audio con el Perceptrón Multicapa II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Este conjunto de datos contiene los discursos de estos líderes prominentes; Benjamín Netanyahu, Jens Stoltenberg, Julia Gillard, Margaret\n",
    "Tacher y Nelson Mandela que también representa los nombres de las carpetas. Cada audio de la carpeta es un PCM de un segundo de duración con una velocidad de muestreo de 16000 hz codificado.\n",
    "Una carpeta llamada background_noise contiene audios que no son discursos, pero que se pueden encontrar en el interior alrededor del entorno del orador, por ejemplo, la audiencia riendo o aplaudiendo. Se puede mezclar con el discurso mientras se entrena.\n",
    "\n",
    "### Referencias\n",
    "https://www.kaggle.com/kongaevans/speaker-recognition-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo del ejercicio consiste en utilizar un perceptrón multicapa para la identificación de la persona que se encuentra hablando a partir de un audio de un segundo de duración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte I: Introducción al procesamiento de audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia debemos comprender como se representa digitalmente el audio y que características fundamentales tiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un conjunto de constantes con las rutas de las carpetas que contienen los audios de cada persona\n",
    "import os\n",
    "\n",
    "DATASET_ROOT = \"16000_pcm_speeches\"\n",
    "BENJAMIN_DATA = os.path.join(DATASET_ROOT, \"Benjamin_Netanyau\")\n",
    "JENS_DATA = os.path.join(DATASET_ROOT, \"Jens_Stoltenberg\")\n",
    "JULIA_DATA = os.path.join(DATASET_ROOT, \"Julia_Gillard\")\n",
    "MARGARET_DATA = os.path.join(DATASET_ROOT, \"Magaret_Tarcher\")\n",
    "NELSON_DATA = os.path.join(DATASET_ROOT, \"Nelson_Mandela\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para procesar el audio vamos a requerir una librería externa que debemos instalar en el contexto de nuestro Jupyter Notebook, concretamente la librería que vamos a utilizar es librosa: https://librosa.org/doc/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escuchamos el fichero de audio que vamos a cargar en el contexto de Jupyter Notebook\n",
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(os.path.join(BENJAMIN_DATA, \"22.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el audio mediante la librería librosa\n",
    "import librosa\n",
    "\n",
    "wav, sr = librosa.load(os.path.join(BENJAMIN_DATA, \"22.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método load de librosa nos devuleve dos resultados:\n",
    "1. Nos devuelve la onda que representa el sonido que estamos cargando en el contexto de nuestro Jupyter Notebook\n",
    "2. Nos devuelve el ratio de sampleo \n",
    "\n",
    "El sonido es una señal analógica, para poder hacerla digital y poderla representar numéricamente tenemos que muestrear la señal original. \n",
    "\n",
    "El muestreo consiste en \"seleccionar\" un número finito de puntos de la señal original y almacenarlos en una matriz. El teorema de muestreo de Nyquist-Shannon mostró que si nuestra tasa de muestreo es lo suficientemente alta, somos capaces de capturar toda la información de la señal e incluso recuperarla completamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tasa de muestreo es muy importante, y la usaremos más adelante en diferentes algoritmos. Generalmente se expresa en hercios (Hz), es decir, el número de puntos (muestras) por segundo. \n",
    "\n",
    "En nuestro ejemplo, sr=22050 por lo que tenemos 22050 muestras por segundo, por lo tanto, podemos calcular la longitud del audio de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con la tasa de muestreo y el tamaño total de la señal se puede calcular la longitud del audio\n",
    "long_audio = len(wav)/sr\n",
    "print(\"La longitud del audio en segundos es:\", long_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las cosas interesantes que debemos hacer para obtener intuiciones sobre nuestra onda de sonido es representarla gráficamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representación gráfica de la onda de sonido\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(wav)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos hacer zoom en determinados puntos específicos seleccionados durante el muestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wav[1000:1200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algo importante que debemos tener en cuenta respecto a lo comentado anteriormente, en realidad, la tasa de muestreo real para el audio que hemos cargado no es 22050 Hz, librosa implícitamente remuestrea nuestros archivos para obtener este valor estándar. Para obtener la tasa de muestreo original, podemos usar el argumento **_sr=False_** en el método **_load_**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = librosa.load(os.path.join(BENJAMIN_DATA, \"22.wav\"), sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tasa de muestreo: {} Hz\".format(sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comenzar intentando utilizar esta representación del sonido para tratar de identificar el orador al que pertenece la grabación mediante el uso de un Perceptrón Multicapa.\n",
    "\n",
    "Hay que tener en cuenta que para este tipo de casos prácticos debemos intentar buscar las voces de los oradores emitidas por diferentes altavoces o dispositivos de sonido para que la clasificación no dependa del altavoz que se esta utilizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función para parsear nuestro conjunto de datos\n",
    "def parse_dataset(dataset_paths):\n",
    "    X = []\n",
    "    y = []\n",
    "    for index, dataset in enumerate(dataset_paths):\n",
    "        print(\"[+] Parsing {} data...\".format(dataset))\n",
    "        for fname in os.listdir(dataset):\n",
    "            wav, sr = librosa.load(os.path.join(dataset, fname), sr=None)\n",
    "            X.append(wav)\n",
    "            y.append(index)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = parse_dataset([BENJAMIN_DATA, JENS_DATA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La longitud del conjunto de datos es: \", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. División del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el conjunto de datos en entrenamiento y pruebas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Longitud del subconjunto de entrenamiento: \", len(X_train))\n",
    "print(\"Longitud del subconjunto de pruebas: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construcción del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos un perceptrón multicapa\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(activation='logistic', hidden_layer_sizes=(10,), solver='sgd')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos la predicción con el conjunto de datos de prueba\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el f1_score resultante de la clasificación\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, y_pred, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte II: Otra representación del sonido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en el ejercicio anterior, si utilizamos las ondas tal cual las estamos leyendo de disco, los resultados de nuestro algoritmo no son demasiado buenos. Por ello, vamos a utilizar otra representación del sonido conocida como espectrogramas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comprender lo que es un espectrograma, debemos comprender lo que son las ondas coseno o cosinusoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "signal = np.cos(np.arange(0, 20, 0.2))\n",
    "\n",
    "plt.plot(signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede observarse que se corresponden con un tipo de ondas sencillas, cuya curva describe una oscilación repetitiva y suave. De manera muy sencilla podemos modificar este patrón cambiando la amplitud y frequencia de la onda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = 2*np.cos(np.arange(0, 20, 0.2)*2)\n",
    "\n",
    "plt.plot(signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos combinar dos ondas de este tipo formando ondas más complejas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos1 = np.cos(np.arange(0, 20, 0.2))\n",
    "cos2 = 2*np.cos(np.arange(0, 20, 0.2)*2)\n",
    "cos3 = 8*np.cos(np.arange(0, 20, 0.2)*4)\n",
    "\n",
    "signal = cos1 + cos2 + cos3\n",
    "\n",
    "plt.plot(signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "cos3 = 8*np.cos(np.arange(0, 20, 0.2)*4)\n",
    "\n",
    "plt.plot(cos3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder proporcionar estas ondas a nuestra Red Neuronal Artificial necesitamos una representación de la misma. Podemos utilizar varias representaciones. \n",
    "1. Almacenar la señal completa, que es considerablemente compleja y que hemos comprobado que no proporciona demasiado buenos resultados\n",
    "2. Podemos almacenar únicamente las frequencias que se utilizan en esta señal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representar únicamente las frecuencias de la señal es más simple y requiere mucho menos espacio en memoria. Aquí tenemos 3 frecuencias diferentes con diferentes amplitudes. La pregunta que debemos hacer es: ¿Cuáles son exactamente esas 3 frecuencias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos un método que, dada una onda digital, nos devuelva las frecuencias en ella. Para ello vamos a utilizar la Transformada de Fourier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos numpy para implementar esta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = np.fft.fft(signal)[:50]\n",
    "fft = np.abs(fft)\n",
    "\n",
    "plt.plot(fft)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos aquí 3 frecuencias: 4, 7 y 14 bits por segundo, exactamente como construimos nuestra señal. Aquí usamos sólo la primera mitad del valor de retorno porque el resultado de la FFT es simétrico.\n",
    "\n",
    "Resulta que cada sonido (incluso el habla humana) está compuesto de muchas de estas ondas coseno básicas en diferentes frecuencias.\n",
    "\n",
    "Tenemos una forma de obtener frecuencias de cualquier señal de sonido, pero el habla humana no es un ruido estático, cambia con el tiempo, así que para representar correctamente el habla humana, descompondremos nuestras grabaciones en pequeñas ventanas y calcularemos qué frecuencias se utilizan en cada ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max)\n",
    "\n",
    "librosa.display.specshow(D, y_axis='linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **_librosa.stft_**: calcula la Transformada de Fourier. Los valores de retorno son una matriz donde X son los números de ventana e Y son las frecuencias. \n",
    "* **_np.abs_**: Toma el absoluto del stft en caso de número complejo devuelve el absoluto de la parte real.\n",
    "* **_librosa.amplitud_a_db_**: Convierte los valores a Decibelios\n",
    "* **_librosa.display.specshow_**: Muestra el espectrograma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espectrograma nos muestra diferentes frecuencias en diferentes partes de la grabación de la voz. Otra de las propiedades fundamentales es que a nivel de representación son muy similares a una imagen en blanco y negro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El espectrograma puede interpretarse como una imagen\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función para parsear nuestro conjunto de datos\n",
    "import librosa\n",
    "\n",
    "def parse_dataset(dataset_paths):\n",
    "    X = []\n",
    "    y = []\n",
    "    for index, dataset in enumerate(dataset_paths):\n",
    "        print(\"[+] Parsing {} data...\".format(dataset))\n",
    "        for fname in os.listdir(dataset):\n",
    "            wav, sr = librosa.load(os.path.join(dataset, fname), sr=None)\n",
    "            D = librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max)\n",
    "            X.append(D)\n",
    "            y.append(index)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_prep, y_prep = parse_dataset([BENJAMIN_DATA, JENS_DATA])\n",
    "X_prep, y_prep = parse_dataset([JENS_DATA, JULIA_DATA, MARGARET_DATA, NELSON_DATA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep[100].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. División del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prep, y_prep, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construcción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamos los subconjuntos de datos para que puedan ser posteriormente procesados por la red neuronal artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = np.array(X_train).reshape((len(X_train), 1025*32))\n",
    "X_train_prep = np.array(X_train_prep).astype('float32') / 255\n",
    "y_train_prep = np.array(y_train)\n",
    "\n",
    "X_test_prep = np.array(X_test).reshape((len(X_test), 1025*32))\n",
    "X_test_prep = np.array(X_test_prep).astype('float32') / 255\n",
    "y_test_prep = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos un perceptrón multicapa\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(activation='logistic', hidden_layer_sizes=(10,), solver='sgd')\n",
    "clf.fit(X_train_prep, y_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos la predicción con el conjunto de datos de prueba\n",
    "y_pred = clf.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el f1_score resultante de la clasificación\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# f1_score(y_test, y_pred, average=\"binary\")\n",
    "f1_score(y_test, y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
